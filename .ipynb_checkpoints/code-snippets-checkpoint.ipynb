{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from math import log\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/afs/inf.ed.ac.uk/user/s24/s2446690/Desktop/anlp/assignment1/data/training.es', '/afs/inf.ed.ac.uk/user/s24/s2446690/Desktop/anlp/assignment1/data/training.en', '/afs/inf.ed.ac.uk/user/s24/s2446690/Desktop/anlp/assignment1/data/training.de']\n"
     ]
    }
   ],
   "source": [
    "data_directory = os.path.join(os.getcwd(), 'data')\n",
    "training_files = []\n",
    "\n",
    "'''\n",
    "Iterates through the current directory to fetch all training files i.e training.de, training.en, training.es\n",
    "Stores the absolute path of these files in the training_files list\n",
    "'''\n",
    "[training_files.append(data_directory + '/' + training_file) for training_file in os.listdir(data_directory) if training_file.startswith('training')]\n",
    "print(training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " vocabulary = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "               'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n",
    "               '0', '.', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "def preprocess_line(line):\n",
    "    # The regex below defines a character set consisting of only alphabets, digits, spaces, full-stops and sentence markers\n",
    "    # Remove all unwanted characters that are not part of this character set\n",
    "    # Additionally replace all digits with 0\n",
    "    # Finally, add beginning and end of sentence markers before returning the line\n",
    "    return '##' + re.sub(r'[^a-z\\d\\s.]+', '', re.sub(r'\\d', '0', line.lower())) + '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 a.\n",
    "def generate_trigrams_from_vocabulary():\n",
    "    # We use the '#' character to symbolise both, the beginning i.e., <s> and end i.e., </s> of a sentence\n",
    "    \n",
    "    # Generate trigrams with the beginning-of-sentence marker i.e., of type <s><s>a\n",
    "    set_of_all_possible_trigrams = ['##' + character for character in vocabulary]\n",
    "    \n",
    "    # Generate all trigrams that are read from the second sentence marker onwards i.e., of type <s>ab\n",
    "    set_of_all_possible_trigrams.extend(['#' + character_one + character_two for character_one in vocabulary for character_two in vocabulary])\n",
    "    \n",
    "    # Generate all trigrams with the end-of-sentence marker i.e. of type ab</s>\n",
    "    set_of_all_possible_trigrams.extend([character_n_minus_one + character_n_minus_two + '#' for character_n_minus_one in vocabulary for character_n_minus_two in vocabulary])\n",
    "    \n",
    "    # Generate all trigrams that occur within the sentence (i.e., everything in between the sentence markers)\n",
    "    set_of_all_possible_trigrams.extend([character_one + character_two + character_three for character_one in vocabulary for character_two in vocabulary for character_three in vocabulary])\n",
    "    \n",
    "    # Return the set of all possible trigrams generated from the given vocabulary\n",
    "    return set_of_all_possible_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 b.\n",
    "def model_trigrams_from_training_data(training_file):\n",
    "    set_of_all_possible_trigrams = generate_trigrams_from_vocabulary()\n",
    "    distribution_over_next_characters = defaultdict()\n",
    "    \n",
    "    # Contruct a map with unique bigram histories as keys and a list of all-possible-next-characters as their value-pair (defined as dict within dict model)\n",
    "    for trigram in set_of_all_possible_trigrams:\n",
    "        bigram = trigram[0:2]\n",
    "        next_character = trigram[2]\n",
    "        if bigram not in distribution_over_next_characters:\n",
    "            distribution_over_next_characters[bigram] = defaultdict()\n",
    "        if next_character not in distribution_over_next_characters[bigram]:\n",
    "            distribution_over_next_characters[bigram][next_character] = 0\n",
    "            \n",
    "    # Read the training file and compute all trigram counts\n",
    "    with open(training_file, 'r') as file:\n",
    "        for line in file:\n",
    "            # Preprocess the data line-wise\n",
    "            preprocessed_line = preprocess_line(line.rstrip())\n",
    "            # Define a dictionary to hold all the trigram counts in the training data\n",
    "            for index in range(len(preprocessed_line) - 2):\n",
    "                trigram = preprocessed_line[index: index + 3]\n",
    "                bigram_history = trigram[0:2]\n",
    "                next_character = trigram[2]\n",
    "                # Prevents inclusion of invalid trigrams like #{char}# and {char}#{char}\n",
    "                if bigram_history in distribution_over_next_characters and next_character in distribution_over_next_characters[bigram_history]:\n",
    "                    distribution_over_next_characters[bigram_history][next_character] += 1\n",
    "    file.close()\n",
    "    \n",
    "    total_num_bigrams = 0\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    # Extract the language suffix from the training file (i.e. en, es or de)\n",
    "    # Create a new file to write out the trained model probabilities for each of the above training files\n",
    "    trigram_model = data_directory + '/' + '{}_trained_trigram_model'.format(training_file[-2:])\n",
    "    \n",
    "    # Compute the probability distributions over the trigrams in the training data and write these out to a file\n",
    "    with open(trigram_model, 'w+') as file:\n",
    "        for bigram_history in distribution_over_next_characters:\n",
    "            # Calculate the total number of trigram instances with the same bigram history\n",
    "            total_num_bigrams = sum(distribution_over_next_characters[bigram_history].values())\n",
    "            for next_character in distribution_over_next_characters[bigram_history]:\n",
    "                # For each bigram in the vocabulary, perform add alpha smoothing over the next possible trigram character\n",
    "                distribution_over_next_characters[bigram_history][next_character] += 1\n",
    "                distribution_over_next_characters[bigram_history][next_character] /= (total_num_bigrams +  vocabulary_size)\n",
    "                file.write(bigram_history + next_character + '\\t' + \"{:.3e}\".format(distribution_over_next_characters[bigram_history][next_character]) + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_file in training_files:\n",
    "    model_trigrams_from_training_data(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 a.\n",
    "def generate_from_LM(language_model, n_characters):\n",
    "    lm_trigram_probabilities = defaultdict()\n",
    "    \n",
    "    # Read the language model\n",
    "    with open(language_model, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line to extract the trigram and its probability, respectively\n",
    "            trigram_and_its_probability = line.rstrip().split('\\t')\n",
    "            \n",
    "            # Unwrap the contents of the list trigram_and_its_probability\n",
    "            trigram = trigram_and_its_probability[0]\n",
    "            trigram_probability = float(trigram_and_its_probability[1])\n",
    "            \n",
    "            # Extract the bigram history and next character for each trigram that's read from the language model\n",
    "            bigram_history = trigram[0:2]\n",
    "            next_character = trigram[2]\n",
    "            \n",
    "            # Store the probabilties for each trigram in a dictionary (using the same dict within dict format as before)\n",
    "            if bigram_history not in lm_trigram_probabilities:\n",
    "                lm_trigram_probabilities[bigram_history] = defaultdict()\n",
    "            if next_character not in lm_trigram_probabilities[bigram_history]:\n",
    "                lm_trigram_probabilities[bigram_history][next_character] = trigram_probability\n",
    "    file.close()\n",
    "    \n",
    "    bigram_history_lookup = '##'\n",
    "    generated_sequence = '##'\n",
    "    \n",
    "    for num_characters in range(n_characters):\n",
    "        # Randomly samples the next character of the trigram sequence, given the bigram history and probability distribution over the trigram vocabulary\n",
    "        distribution = lm_trigram_probabilities[bigram_history_lookup]\n",
    "        all_possible_characters = np.array(list(distribution.keys()))\n",
    "        probablity_distribution = np.array(list(distribution.values()))\n",
    "        bins = np.cumsum(probablity_distribution)\n",
    "        chosen_next_character = ''.join(map(str, all_possible_characters[np.digitize(np.random.random_sample(1), bins)]))\n",
    "        if chosen_next_character == '#':\n",
    "            bigram_history_lookup = '##'\n",
    "        else:\n",
    "            bigram_history_lookup = bigram_history_lookup[1] + chosen_next_character\n",
    "        generated_sequence += chosen_next_character\n",
    "        num_characters += 1\n",
    "    \n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 b.\n",
    "def write_generated_sequence_to_file(sequence, language_model):\n",
    "    with open ('{}_generated_sequence'.format(language_model), 'w+') as file:\n",
    "        for character in sequence:\n",
    "            if character == '#':\n",
    "                continue\n",
    "            else:\n",
    "                file.write(character)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 c.\n",
    "# For the English model trained by us\n",
    "language_model = data_directory + '/' + 'en_trained_trigram_model'\n",
    "generated_sequence = generate_from_LM(language_model, 300)\n",
    "write_generated_sequence_to_file(generated_sequence, language_model)\n",
    "\n",
    "# For the pre-trained English model model-br.en\n",
    "language_model = data_directory + '/' + 'model-br.en'\n",
    "generated_sequence = generate_from_LM(language_model, 300)\n",
    "write_generated_sequence_to_file(generated_sequence, language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5 a.\n",
    "def compute_perplexity(language_model, test_document):\n",
    "    lm_trigram_probabilities = defaultdict()\n",
    "    \n",
    "    # Read the language model\n",
    "    with open(language_model, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line to extract the trigram and its probability, respectively\n",
    "            trigram_and_its_probability = line.rstrip().split('\\t')\n",
    "            \n",
    "            # Unwrap the contents of the list trigram_and_its_probability\n",
    "            trigram = trigram_and_its_probability[0]\n",
    "            trigram_probability = float(trigram_and_its_probability[1])\n",
    "            \n",
    "            # Extract the bigram history and next character for each trigram that's read from the language model\n",
    "            bigram_history = trigram[0:2]\n",
    "            next_character = trigram[2]\n",
    "            \n",
    "            # Store the probabilties for each trigram in a dictionary (using the same dict within dict format as before)\n",
    "            if bigram_history not in lm_trigram_probabilities:\n",
    "                lm_trigram_probabilities[bigram_history] = defaultdict()\n",
    "            if next_character not in lm_trigram_probabilities[bigram_history]:\n",
    "                lm_trigram_probabilities[bigram_history][next_character] = trigram_probability\n",
    "    file.close()\n",
    "    \n",
    "    sum_of_log_probabilties = float(0)\n",
    "    hm = float(0)\n",
    "    N = float(0)\n",
    "    \n",
    "    with open(test_document, 'r') as file:\n",
    "        for line in file:\n",
    "            preprocessed_line = preprocess_line(line.rstrip())\n",
    "            # Exclude the first two '##' in each line for calculating the total number of character tokens in the file\n",
    "            N += len(preprocessed_line) - 2\n",
    "            for index in range(len(preprocessed_line) - 2):\n",
    "                trigram = preprocessed_line[index: index + 3]\n",
    "                bigram_history = trigram[0:2]\n",
    "                next_character = trigram[2]\n",
    "                sum_of_log_probabilties += math.log2(lm_trigram_probabilities[bigram_history][next_character])\n",
    "    file.close()\n",
    "    \n",
    "    hm = (-1.0/N) * sum_of_log_probabilties\n",
    "    perplexity = math.pow(2, hm)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5 b.\n",
    "test_data = data_directory + '/' + 'test'\n",
    "\n",
    "language_model = data_directory + '/' + 'en_trained_trigram_model'\n",
    "compute_perplexity(language_model, test_data)\n",
    "\n",
    "language_model = data_directory + '/' + 'es_trained_trigram_model'\n",
    "compute_perplexity(language_model, test_data)\n",
    "\n",
    "language_model = data_directory + '/' + 'de_trained_trigram_model'\n",
    "compute_perplexity(language_model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
